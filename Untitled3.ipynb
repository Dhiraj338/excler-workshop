{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNzskxCmHYHx0V+lsK4hToI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhiraj338/excler-workshop/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i21rQA2z66gT",
        "outputId": "4d7078d7-dec4-4008-9a78-06fced30502e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value: 42 | Type: <class 'int'>\n",
            "Value: 3.14 | Type: <class 'float'>\n",
            "Value: Hello, Jupyter! | Type: <class 'str'>\n",
            "Value: True | Type: <class 'bool'>\n"
          ]
        }
      ],
      "source": [
        "# Integer\n",
        "my_integer = 42\n",
        "print(\"Value:\", my_integer, \"| Type:\", type(my_integer))\n",
        "\n",
        "# Float\n",
        "my_float = 3.14\n",
        "print(\"Value:\", my_float, \"| Type:\", type(my_float))\n",
        "\n",
        "# String\n",
        "my_string = \"Hello, Jupyter!\"\n",
        "print(\"Value:\", my_string, \"| Type:\", type(my_string))\n",
        "\n",
        "# Boolean\n",
        "my_boolean = True\n",
        "print(\"Value:\", my_boolean, \"| Type:\", type(my_boolean))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List with 5 elements\n",
        "my_list = [10, 20, 30, 40, 50]\n",
        "print(\"List:\", my_list)\n",
        "print(\"Element at index 0:\", my_list[0])\n",
        "print(\"Element at index 3:\", my_list[3])\n",
        "print(\"Slice from index 1 to 3:\", my_list[1:4])  # Slicing the list\n",
        "\n",
        "# Tuple with 5 elements\n",
        "my_tuple = (\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\")\n",
        "print(\"\\nTuple:\", my_tuple)\n",
        "print(\"Element at index 2:\", my_tuple[2])\n",
        "print(\"Last element:\", my_tuple[-1])  # Accessing with negative index\n",
        "\n",
        "# Dictionary with 5 key-value pairs\n",
        "my_dict = {\n",
        "    \"name\": \"Alice\",\n",
        "    \"age\": 25,\n",
        "    \"city\": \"New York\",\n",
        "    \"job\": \"Engineer\",\n",
        "    \"hobby\": \"Reading\"\n",
        "}\n",
        "print(\"\\nDictionary:\", my_dict)\n",
        "print(\"Value for 'name':\", my_dict[\"name\"])\n",
        "print(\"Value for 'hobby':\", my_dict[\"hobby\"])\n",
        "print(\"Value for 'job':\", my_dict.get(\"job\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-t2AUE17XKQ",
        "outputId": "cba6c0fe-af64-49d8-c7f2-53a6cfa64a32"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List: [10, 20, 30, 40, 50]\n",
            "Element at index 0: 10\n",
            "Element at index 3: 40\n",
            "Slice from index 1 to 3: [20, 30, 40]\n",
            "\n",
            "Tuple: ('apple', 'banana', 'cherry', 'date', 'elderberry')\n",
            "Element at index 2: cherry\n",
            "Last element: elderberry\n",
            "\n",
            "Dictionary: {'name': 'Alice', 'age': 25, 'city': 'New York', 'job': 'Engineer', 'hobby': 'Reading'}\n",
            "Value for 'name': Alice\n",
            "Value for 'hobby': Reading\n",
            "Value for 'job': Engineer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizes a sample paragraph into words and sentences.\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')  # Optional for other tasks\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Re-download the 'punkt' package to ensure all files are intact\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Sample paragraph\n",
        "paragraph = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field of artificial intelligence.\n",
        "It helps machines understand, interpret, and respond to human language. Tokenization is one of the first steps in NLP!\n",
        "\"\"\"\n",
        "\n",
        "# Sentence Tokenization\n",
        "try:\n",
        "    sentences = sent_tokenize(paragraph)\n",
        "    print(\"Sentence Tokenization:\")\n",
        "    for i, sentence in enumerate(sentences, 1):\n",
        "        print(f\"{i}. {sentence}\")\n",
        "except LookupError as e:\n",
        "    print(\"Error during sentence tokenization:\", e)\n",
        "\n",
        "# Word Tokenization\n",
        "try:\n",
        "    words = word_tokenize(paragraph)\n",
        "    print(\"\\nWord Tokenization:\")\n",
        "    print(words)\n",
        "except LookupError as e:\n",
        "    print(\"Error during word tokenization:\", e)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSGPrTeT70uh",
        "outputId": "937ae0e8-ccc5-4d21-aadd-d6e33dbeeab6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization:\n",
            "1. \n",
            "Natural Language Processing (NLP) is a fascinating field of artificial intelligence.\n",
            "2. It helps machines understand, interpret, and respond to human language.\n",
            "3. Tokenization is one of the first steps in NLP!\n",
            "\n",
            "Word Tokenization:\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'artificial', 'intelligence', '.', 'It', 'helps', 'machines', 'understand', ',', 'interpret', ',', 'and', 'respond', 'to', 'human', 'language', '.', 'Tokenization', 'is', 'one', 'of', 'the', 'first', 'steps', 'in', 'NLP', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program that takes a student's marks in three subjects as input.\n",
        "  #If the average is greater than or equal to 90, print \"Grade: A\".\n",
        "  #If the average is between 80 and 89, print \"Grade: B\".\n",
        "  #If the average is between 70 and 79, print \"Grade: C\".\n",
        "  #Otherwise, print \"Grade: Fail\".\n",
        "\n",
        "# Taking marks for three subjects as input\n",
        "subject1 = float(input(\"Enter marks for Subject 1: \"))\n",
        "subject2 = float(input(\"Enter marks for Subject 2: \"))\n",
        "subject3 = float(input(\"Enter marks for Subject 3: \"))\n",
        "\n",
        "# Calculating the average\n",
        "average = (subject1 + subject2 + subject3) / 3\n",
        "\n",
        "# Determining the grade based on the average\n",
        "if average >= 90:\n",
        "    print(\"Grade: A\")\n",
        "elif 80 <= average < 90:\n",
        "    print(\"Grade: B\")\n",
        "elif 70 <= average < 80:\n",
        "    print(\"Grade: C\")\n",
        "else:\n",
        "    print(\"Grade: Fail\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rCezBIr8qH5",
        "outputId": "fa29793c-64b9-47e2-e6d1-6785c0905151"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter marks for Subject 1: 3\n",
            "Enter marks for Subject 2: 4\n",
            "Enter marks for Subject 3: 5\n",
            "Grade: Fail\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking input for the positive integer n\n",
        "n = int(input(\"Enter a positive integer: \"))\n",
        "\n",
        "# Initialize the sum\n",
        "even_sum = 0\n",
        "\n",
        "# Calculate the sum of all even numbers between 1 and n\n",
        "for i in range(2, n + 1, 2):  # Start at 2 and increment by 2\n",
        "    even_sum += i\n",
        "\n",
        "# Print the result\n",
        "print(f\"The sum of all even numbers between 1 and {n} is: {even_sum}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTzTdcdO8q4g",
        "outputId": "cf895e68-4632-4a62-9162-70edb258fd3d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a positive integer: 20\n",
            "The sum of all even numbers between 1 and 20 is: 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking input text from the user\n",
        "text = input(\"Enter a text: \")\n",
        "\n",
        "# Converting the text to lowercase and splitting it into words\n",
        "words = text.lower().split()\n",
        "\n",
        "# Initializing an empty dictionary to store word frequencies\n",
        "word_counts = {}\n",
        "\n",
        "# Counting the frequency of each word\n",
        "for word in words:\n",
        "    word_counts[word] = word_counts.get(word, 0) + 1\n",
        "\n",
        "# Printing the words and their counts\n",
        "print(\"\\nWord Frequencies:\")\n",
        "for word, count in word_counts.items():\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUlMjrQn88oO",
        "outputId": "2eba2ad0-3be1-4dc7-f1de-97b40bd9eae8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text: hii\n",
            "\n",
            "Word Frequencies:\n",
            "hii: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to using NLTK and Spacy\n",
        "  #Convert text to lowercase.\n",
        "  #Remove stopwords using NLTK\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input text\n",
        "text = \"This is a simple example to demonstrate how to process text using NLTK and spaCy.\"\n",
        "\n",
        "# Convert text to lowercase using spaCy\n",
        "doc = nlp(text.lower())\n",
        "\n",
        "# Remove stopwords using NLTK\n",
        "filtered_words = [word.text for word in doc if word.text not in stop_words and word.is_alpha]\n",
        "\n",
        "# Print the processed text\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Processed Text:\", \" \".join(filtered_words))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8ICrEdz9fvn",
        "outputId": "c58550b8-636b-4f21-e460-158b5224f42a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: This is a simple example to demonstrate how to process text using NLTK and spaCy.\n",
            "Processed Text: simple example demonstrate process text using nltk spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gensim nltk\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Load sample text file\n",
        "#file_path = \"sample_text.rtf\"  # Replace with your text file path\n",
        "# Instead of reading from file, define sample text directly\n",
        "text = \"\"\"This is a sample text for demonstration.\n",
        "          It contains multiple sentences and words.\n",
        "          We will apply preprocessing steps like tokenization,\n",
        "          stop word removal, stemming, and lemmatization.\"\"\"\n",
        "\n",
        "\n",
        "#with open(file_path, \"r\") as file:\n",
        "#    text = file.read()\n",
        "\n",
        "# Preprocessing: Tokenization using Gensim\n",
        "tokens = simple_preprocess(text, deacc=True)  # deacc=True removes punctuation\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# Stemming\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word, wordnet.VERB) for word in stemmed_tokens]\n",
        "\n",
        "# Display results\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nTokenized Text:\")\n",
        "print(tokens)\n",
        "print(\"\\nAfter Removing Stopwords:\")\n",
        "print(filtered_tokens)\n",
        "print(\"\\nAfter Stemming:\")\n",
        "print(stemmed_tokens)\n",
        "print(\"\\nAfter Lemmatization:\")\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6v_xDkE9lTD",
        "outputId": "7f2aa9c3-fcca-4898-e382-7515baf81428"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Original Text:\n",
            "This is a sample text for demonstration. \n",
            "          It contains multiple sentences and words. \n",
            "          We will apply preprocessing steps like tokenization, \n",
            "          stop word removal, stemming, and lemmatization.\n",
            "\n",
            "Tokenized Text:\n",
            "['this', 'is', 'sample', 'text', 'for', 'demonstration', 'it', 'contains', 'multiple', 'sentences', 'and', 'words', 'we', 'will', 'apply', 'preprocessing', 'steps', 'like', 'tokenization', 'stop', 'word', 'removal', 'stemming', 'and', 'lemmatization']\n",
            "\n",
            "After Removing Stopwords:\n",
            "['sample', 'text', 'demonstration', 'contains', 'multiple', 'sentences', 'words', 'apply', 'preprocessing', 'steps', 'like', 'tokenization', 'stop', 'word', 'removal', 'stemming', 'lemmatization']\n",
            "\n",
            "After Stemming:\n",
            "['sampl', 'text', 'demonstr', 'contain', 'multipl', 'sentenc', 'word', 'appli', 'preprocess', 'step', 'like', 'token', 'stop', 'word', 'remov', 'stem', 'lemmat']\n",
            "\n",
            "After Lemmatization:\n",
            "['sampl', 'text', 'demonstr', 'contain', 'multipl', 'sentenc', 'word', 'appli', 'preprocess', 'step', 'like', 'token', 'stop', 'word', 'remov', 'stem', 'lemmat']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters using regular expressions (retain only alphabets and spaces)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test the function with the input text\n",
        "input_text = \"Hello, World! Welcome to NLP 101.\"\n",
        "cleaned_text = clean_text(input_text)\n",
        "\n",
        "print(\"Original Text:\", input_text)\n",
        "print(\"Cleaned Text:\", cleaned_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhK0FFJM9uWg",
        "outputId": "12aea2e9-d673-4175-fcc7-b6ef209de345"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Hello, World! Welcome to NLP 101.\n",
            "Cleaned Text: hello world welcome to nlp \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters using regular expressions (retain only alphabets and spaces)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test the function with the input text\n",
        "input_text = \"Hello, World! Welcome to NLP 101.\"\n",
        "cleaned_text = clean_text(input_text)\n",
        "\n",
        "print(\"Original Text:\", input_text)\n",
        "print(\"Cleaned Text:\", cleaned_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftWJ6L-i-FpN",
        "outputId": "2d587848-4d68-4468-b03c-82d1b40f7f40"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Hello, World! Welcome to NLP 101.\n",
            "Cleaned Text: hello world welcome to nlp \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4\n",
        "\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Define the URL\n",
        "url = 'https://example.com'\n",
        "\n",
        "# Fetch the content of the webpage\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find and print the title of the webpage\n",
        "    title = soup.title.string\n",
        "    print(\"Title of the webpage:\", title)\n",
        "else:\n",
        "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
        "\n",
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Define the URL\n",
        "url = 'https://example.com'\n",
        "\n",
        "# Fetch the content of the webpage\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find and print the title of the webpage\n",
        "    title = soup.title.string\n",
        "    print(\"Title of the webpage:\", title)\n",
        "else:\n",
        "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
        "\n",
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Define the URL\n",
        "url = 'https://example.com'\n",
        "\n",
        "# Fetch the content of the webpage\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find and print the title of the webpage\n",
        "    title = soup.title.string\n",
        "    print(\"Title of the webpage:\", title)\n",
        "else:\n",
        "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
        "\n",
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Define the URL\n",
        "url = 'https://example.com'\n",
        "\n",
        "# Fetch the content of the webpage\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find and print the title of the webpage\n",
        "    title = soup.title.string\n",
        "    print(\"Title of the webpage:\", title)\n",
        "else:\n",
        "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gnuLwsM-KJw",
        "outputId": "73a8eaae-0dff-4b6f-9602-4ee18735baf5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Title of the webpage: Example Domain\n",
            "Title of the webpage: Example Domain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install requests beautifulsoup4\n",
        "\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Define the URL\n",
        "url = 'https://example.com'\n",
        "\n",
        "# Fetch the content of the webpage\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find and print the title of the webpage\n",
        "    title = soup.title.string\n",
        "    print(\"Title of the webpage:\", title)\n",
        "else:\n",
        "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO3OG0Ii-OzF",
        "outputId": "49d04bfc-db31-4794-9ce0-69f30ffec840"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Title of the webpage: Example Domain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IVIqMq-U-fIC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}